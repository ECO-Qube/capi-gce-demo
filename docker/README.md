## Setting up Cluster API on Docker 

### Setting up Cluster API using KinD/Docker

From https://cluster-api.sigs.k8s.io/user/quick-start.html

Run the following command to create a kind config file for allowing the Docker provider to access Docker on the host
(file already present):

```bash
cat > kind-cluster-with-extramounts.yaml <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraMounts:
    - hostPath: /var/run/docker.sock
      containerPath: /var/run/docker.sock
EOF
```

Environment variables to set:

```bash
# Enable the experimental Cluster topology feature.
export CLUSTER_TOPOLOGY=true

# Enable ClusterResourceSet experimental feature
export EXP_CLUSTER_RESOURCE_SET=true
```

The Docker provider does not require additional configurations for cluster templates, however things like CIDRs and 
Service Domain name are configurable so check out the quickstart if necessary.

Init KinD cluster:

```bash
kind create cluster --config kind-cluster-with-extramounts.yaml  
```

Init the provider:

```bash
clusterctl init --infrastructure docker
```

Note cluster manifests are already present, so just run the following:

```bash
kubectl apply -f ecoqube-dev.yaml
```

then apply all configmaps for CRS, CRS templates and cluster resource:

```bash
kubectl apply -f "capi-resource-set/manifests/*-configmap.yaml"
kubectl apply -f clusterresourcesets.yaml 
kubectl apply -f ecoqube-dev-cluster.yaml
```

<details open>
<summary>else if starting from scratch, generate the cluster YAML configs</summary>
<br>

```bash
clusterctl generate cluster ecoqube-dev --flavor development \
  --kubernetes-version v1.25.3 \
  --control-plane-machine-count=1 \
  --worker-machine-count=3 \
  > ecoqube-dev.yaml
```

Modify the yaml with your desired config. Note at this time a PR is pending to correct an issue with the cluster template
generated by clusterctl. See [this Slack thread for details](https://kubernetes.slack.com/archives/C8TSNPY4T/p1668525942617209).

Separate the cluster resource in its separate file, e.g. `ecoqube-dev-cluster.yaml`.

When ready:

```bash
kubectl apply -f ecoqube-dev.yaml
kubectl apply -f [.. configmaps, cluster resource set ..]
kubectl apply -f ecoqube-dev-cluster.yaml
```
</details>



Wait until the control plane is up and running using the following command:

```bash
watch -n 1 kubectl get kubeadmcontrolplane
```

Check if control plane is ready: both INITIALIZED API SERVER and API SERVER AVAILABLE must be true.
Cluster should be ready in just a minute or two.

```bash
clusterctl get kubeconfig ecoqube-dev > ecoqube-dev.kubeconfig
```

Important detail for Linux and macOS users: [you'll need an extra step to get the kubeconfig right](https://cluster-api.sigs.k8s.io/clusterctl/developers.html#additional-notes-for-the-docker-provider). Execute this to point
to the right address for connecting to the cluster, else you'll get a connection refused error:

```bash
# Point the kubeconfig to the exposed port of the load balancer, rather than the inaccessible container IP.
sed -i -e "s/server:.*/server: https:\/\/$(docker port capi-quickstart-lb 6443/tcp | sed "s/0.0.0.0/127.0.0.1/")/g" ./capi-quickstart.kubeconfig
```

Check nodes for Ready status:

```bash
kubectl --kubeconfig=./ecoqube-dev.kubeconfig get nodes
```

### Additional notes about setting up TAS in CAPD

Note the difference between CAPD and the other infra providers, CAPD uses the new `ClusterClass` resource.
It's different than the providers right now because it uses `KubeadmControlPlaneTemplate` vs the plain
`KubeadmControlPlane` resource. Patches are not present under `initConfiguration` and `joinConfiguration`.